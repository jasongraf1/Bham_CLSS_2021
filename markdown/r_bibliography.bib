
@incollection{aarts_corpus_2000,
  title = {Corpus Linguistics, {{Chomsky}} and Fuzzy Tree Fragments},
  booktitle = {Corpus Linguistics and Linguistic Theory},
  author = {Aarts, Bas},
  editor = {Mair, Christian and Hundt, Marianne},
  year = {2000},
  pages = {5--13},
  publisher = {{Rodopi}},
  address = {{Amsterdam}},
  isbn = {978-90-420-1493-0},
  keywords = {Chomsky,corpus-linguistics,empiricism}
}

@article{altmann_permutation_2010,
  title = {Permutation Importance: A Corrected Feature Importance Measure},
  shorttitle = {Permutation Importance},
  author = {Altmann, Andr{\'e} and Tolo{\c s}i, Laura and Sander, Oliver and Lengauer, Thomas},
  year = {2010},
  month = may,
  volume = {26},
  pages = {1340--1347},
  issn = {1367-4803},
  doi = {10/cm7h6d},
  abstract = {Abstract.  Motivation: In life sciences, interpretability of machine learning models is as important as their prediction accuracy. Linear models are probably th},
  annotation = {00137},
  file = {C\:\\Users\\grafmilj\\Zotero\\storage\\LJWPWI26\\Altmann et al. - 2010 - Permutation importance a corrected feature import.pdf;C\:\\Users\\grafmilj\\Zotero\\storage\\XD3A8SR5\\193348.html},
  journal = {Bioinformatics},
  keywords = {random forests},
  language = {en},
  number = {10}
}

@article{bernaisch_dative_2014,
  title = {The Dative Alternation in {{South Asian English}}(Es): {{Modelling}} Predictors and Predicting Prototypes},
  author = {Bernaisch, Tobias and Gries, Stefan Th. and Mukherjee, Joybrato},
  year = {2014},
  volume = {35},
  pages = {7--31},
  file = {C\:\\Users\\grafmilj\\Zotero\\storage\\7WGHGQZ9\\Bernaisch_et_al_ta-DatAltInSAsEngl_EWW.pdf},
  journal = {English World-Wide},
  keywords = {Comparative sociolinguistics,datives,English,random forests,world-Englishes}
}

@article{boulesteix_letter_2015,
  title = {Letter to the {{Editor}}: {{On}} the Term 'interaction' and Related Phrases in the Literature on {{Random Forests}}},
  shorttitle = {Letter to the {{Editor}}},
  author = {Boulesteix, A.-L. and Janitza, S. and Hapfelmeier, A. and Van Steen, K. and Strobl, C.},
  year = {2015},
  month = mar,
  volume = {16},
  pages = {338--345},
  issn = {1467-5463, 1477-4054},
  doi = {10/f67gf3},
  annotation = {00023},
  journal = {Briefings in Bioinformatics},
  keywords = {random forests},
  language = {en},
  number = {2}
}

@book{breiman_classification_1998,
  title = {Classification and Regression Trees},
  editor = {Breiman, Leo},
  year = {1998},
  edition = {Repr},
  publisher = {{Chapman \& Hall [u.a.]}},
  address = {{Boca Raton}},
  annotation = {48474  OCLC: 247053926},
  isbn = {978-0-412-04841-8},
  keywords = {CARTs,statistical methods},
  language = {eng}
}

@article{breiman_random_2001,
  title = {Random {{Forests}}},
  author = {Breiman, Leo},
  year = {2001},
  volume = {41},
  pages = {5--32},
  annotation = {67809},
  journal = {Machine Learning},
  keywords = {modeling,random forests,statistics}
}

@article{debeer_conditional_2020,
  title = {Conditional Permutation Importance Revisited},
  author = {Debeer, Dries and Strobl, Carolin},
  year = {2020},
  month = dec,
  volume = {21},
  pages = {307},
  issn = {1471-2105},
  doi = {10.1186/s12859-020-03622-2},
  annotation = {00001},
  file = {C\:\\Users\\grafmilj\\Zotero\\storage\\9CKRT6YD\\Debeer and Strobl - 2020 - Conditional permutation importance revisited.pdf},
  journal = {BMC Bioinformatics},
  keywords = {random forests,statistical methods,Tree based methods},
  language = {en},
  number = {1}
}

@book{desagulier_corpus_2017,
  title = {Corpus Linguistics and Statistics with {{R}}: {{Introduction}} to Quantitative Methods in Linguistics},
  shorttitle = {Corpus {{Linguistics}} and {{Statistics}} with {{R}}},
  author = {Desagulier, Guillaume},
  year = {2017},
  edition = {1st ed. 2017},
  publisher = {{Springer International Publishing : Imprint: Springer}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-64572-8},
  abstract = {This textbook examines empirical linguistics from a theoretical linguist's perspective. It provides both a theoretical discussion of what quantitative corpus linguistics entails and detailed, hands-on, step-by-step instructions to implement the techniques in the field. The statistical methodology and R-based coding from this book teach readers the basic and then more advanced skills to work with large data sets in their linguistics research and studies. Massive data sets are now more than ever the basis for work that ranges from usage-based linguistics to the far reaches of applied linguistics. This book presents much of the methodology in a corpus-based approach. However, the corpus-based methods in this book are also essential components of recent developments in sociolinguistics, historical linguistics, computational linguistics, and psycholinguistics. Material from the book will also be appealing to researchers in digital humanities and the many non-linguistic fields that use textual data analysis and text-based sensorimetrics. Chapters cover topics including corpus processing, frequencing data, and clustering methods. Case studies illustrate each chapter with accompanying data sets, R code, and exercises for use by readers. This book may be used in advanced undergraduate courses, graduate courses, and self-study},
  annotation = {00000},
  isbn = {978-3-319-64572-8},
  keywords = {Computational linguistics,Computational Linguistics,Grammar,Statistics,Statistics and Computing/Statistics Programs,Statistics for Social Sciences; Humanities; Law,textbooks},
  lccn = {519.5},
  series = {Quantitative {{Methods}} in the {{Humanities}} and {{Social Sciences}}}
}

@article{goldstein_peeking_2015,
  title = {Peeking inside the Black Box: Visualizing Statistical Learning with Plots of {{Individual Conditional Expectation}}},
  shorttitle = {Peeking {{Inside}} the {{Black Box}}},
  author = {Goldstein, Alex and Kapelner, Adam and Bleich, Justin and Pitkin, Emil},
  year = {2015},
  month = jan,
  volume = {24},
  pages = {44--65},
  issn = {1061-8600},
  doi = {10/gffgnc},
  abstract = {This article presents individual conditional expectation (ICE) plots, a tool for visualizing the model estimated by any supervised learning algorithm. Classical partial dependence plots (PDPs) help visualize the average partial relationship between the predicted response and one or more features. In the presence of substantial interaction effects, the partial response relationship can be heterogeneous. Thus, an average curve, such as the PDP, can obfuscate the complexity of the modeled relationship. Accordingly, ICE plots refine the PDP by graphing the functional relationship between the predicted response and the feature for individual observations. Specifically, ICE plots highlight the variation in the fitted values across the range of a covariate, suggesting where and to what extent heterogeneities might exist. In addition to providing a plotting suite for exploratory analysis, we include a visual test for additive structure in the data-generating model. Through simulated examples and real datasets, we demonstrate how ICE plots can shed light on estimated models in ways PDPs cannot. Procedures outlined are available in the R package ICEbox.},
  annotation = {00000},
  file = {C\:\\Users\\grafmilj\\Zotero\\storage\\XX9S2HNV\\10618600.2014.html},
  journal = {Journal of Computational and Graphical Statistics},
  keywords = {Exploratory data analysis,Graphical method,Model visualization,random forests},
  number = {1}
}

@article{gries_classification_2019,
  title = {On Classification Trees and Random Forests in Corpus Linguistics: {{Some}} Words of Caution and Suggestions for Improvement},
  shorttitle = {On Classification Trees and Random Forests in Corpus Linguistics},
  author = {Gries, Stefan Th.},
  year = {2019},
  volume = {0},
  issn = {1613-7027},
  doi = {10.1515/cllt-2018-0078},
  abstract = {This paper is a discussion of methodological problems that (can) arise in the analysis of multifactorial data analyzed with tree-based or forest-based classifiers in (corpus) linguistics. I showcase a data set that highlights where such methods can fail at providing optimal results and then discuss solutions to this problem as well as the interpretation of random forests more generally.},
  annotation = {00022},
  journal = {Corpus Linguistics and Linguistic Theory},
  keywords = {CARTs,classification trees,decision tree,random forests,regression modeling},
  number = {0}
}

@article{hajjem_generalized_2017,
  title = {Generalized Mixed Effects Regression Trees},
  author = {Hajjem, Ahlem and Larocque, Denis and Bellavance, Fran{\c c}ois},
  year = {2017},
  month = jul,
  volume = {126},
  pages = {114--118},
  issn = {0167-7152},
  doi = {10/gdthn9},
  abstract = {This paper presents generalized mixed effects regression trees, an extension of mixed effects regression trees to other types of outcomes. A simulation shows that the proposed method provides substantial improvements over standard trees when data are correlated.},
  annotation = {00004},
  file = {C\:\\Users\\grafmilj\\Zotero\\storage\\QNKIUW46\\S0167715217300895.html},
  journal = {Statistics \& Probability Letters},
  keywords = {CARTs,Clustered data,EM-algorithm,Mixed effects,Mixed-effects modeling,Penalized quasi-likelihood algorithm,random forests,to read,Tre,Tree based methods}
}

@article{hajjem_mixedeffects_2014,
  title = {Mixed-Effects Random Forest for Clustered Data},
  author = {Hajjem, Ahlem and Bellavance, Fran{\c c}ois and Larocque, Denis},
  year = {2014},
  month = jun,
  volume = {84},
  pages = {1313--1328},
  issn = {0094-9655},
  doi = {10.1080/00949655.2012.741599},
  abstract = {This paper presents an extension of the random forest (RF) method to the case of clustered data. The proposed `mixed-effects random forest' (MERF) is implemented using a standard RF algorithm within the framework of the expectation\textendash maximization algorithm. Simulation results show that the proposed MERF method provides substantial improvements over standard RF when the random effects are non-negligible. The use of the method is illustrated to predict the first-week box office revenues of movies.},
  annotation = {00022},
  file = {C\:\\Users\\grafmilj\\Zotero\\storage\\J449H4J6\\00949655.2012.html},
  journal = {Journal of Statistical Computation and Simulation},
  keywords = {CARTs,clustered data,Mixed-effects modeling,random forests,to read,Tree based methods},
  number = {6}
}

@book{hastie_elements_2009,
  title = {The Elements of Statistical Learning: {{Data}} Mining, Inference, and Prediction},
  shorttitle = {The Elements of Statistical Learning},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, J. H.},
  year = {2009},
  edition = {2nd ed},
  publisher = {{Springer}},
  address = {{New York, NY}},
  isbn = {978-0-387-84857-0 978-0-387-84858-7},
  keywords = {Bioinformatics,CARTs,Computational intelligence,Data mining,Forecasting,Inference,Machine learning,methodology,statistics,textbooks,to read},
  lccn = {Q325.5 .H39 2009},
  series = {Springer Series in Statistics}
}

@article{hothorn_survival_2005,
  title = {Survival Ensembles},
  author = {Hothorn, T.},
  year = {2005},
  month = dec,
  volume = {7},
  pages = {355--373},
  issn = {1465-4644, 1468-4357},
  doi = {10.1093/biostatistics/kxj011},
  journal = {Biostatistics},
  keywords = {R (Computer program language),R-package,random forests},
  language = {en},
  number = {3}
}

@article{janitza_aucbased_2013,
  title = {An {{AUC}}-Based Permutation Variable Importance Measure for Random Forests},
  author = {Janitza, Silke and Strobl, Carolin and Boulesteix, Anne-Laure},
  year = {2013},
  volume = {14},
  pages = {119},
  issn = {1471-2105},
  doi = {10/f4s4cx},
  annotation = {00075},
  journal = {BMC Bioinformatics},
  keywords = {random forests,statistical methods},
  language = {en},
  number = {1}
}

@article{janitza_computationally_2016,
  title = {A Computationally Fast Variable Importance Test for Random Forests for High-Dimensional Data},
  author = {Janitza, Silke and Celik, Ender and Boulesteix, Anne-Laure},
  year = {2016},
  month = nov,
  issn = {1862-5347, 1862-5355},
  doi = {10/gdj8zn},
  annotation = {00075},
  journal = {Advances in Data Analysis and Classification},
  keywords = {random forests},
  language = {en}
}

@article{janitza_random_2016,
  title = {Random Forest for Ordinal Responses: {{Prediction}} and Variable Selection},
  shorttitle = {Random Forest for Ordinal Responses},
  author = {Janitza, Silke and Tutz, Gerhard and Boulesteix, Anne-Laure},
  year = {2016},
  month = apr,
  volume = {96},
  pages = {57--73},
  issn = {0167-9473},
  doi = {10/gdthn6},
  abstract = {The random forest method is a commonly used tool for classification with high-dimensional data that is able to rank candidate predictors through its inbuilt variable importance measures. It can be applied to various kinds of regression problems including nominal, metric and survival response variables. While classification and regression problems using random forest methodology have been extensively investigated in the past, in the case of ordinal response there is no standard procedure. Extensive studies using random forest based on conditional inference trees are conducted to explore whether incorporating the ordering information yields any improvement in both prediction performance or variable selection. Two novel permutation variable importance measures are presented that are reasonable alternatives to the currently implemented importance measure which was developed for nominal response and makes no use of the ordering in the levels of an ordinal response variable. Results based on simulated and real data suggest that predictor rankings can be improved in some settings by using new permutation importance measures that explicitly use the ordering in the response levels in combination with ordinal regression trees. With respect to prediction accuracy, the performance of ordinal regression trees was similar to and in most settings even slightly better than that of classification trees.},
  annotation = {00029},
  file = {C\:\\Users\\grafmilj\\Zotero\\storage\\E7R874S3\\S0167947315002601.html},
  journal = {Computational Statistics \& Data Analysis},
  keywords = {Feature selection,Ordinal regression trees,Ordinal response,Prediction,random forests,Variable importance}
}

@book{levshina_how_2015,
  title = {How to {{Do Linguistics}} with {{R}}: {{Data Exploration}} and {{Statistical Analysis}}},
  shorttitle = {How to {{Do Linguistics}} with {{R}}},
  author = {Levshina, Natalia},
  year = {2015},
  publisher = {{John Benjamins Publishing Company}},
  address = {{Amsterdam ; Philadelphia}},
  annotation = {00420},
  isbn = {978-90-272-1224-5 978-90-272-1225-2},
  keywords = {Computational linguistics,Linguistics,methodology,Software,statistical methods,textbooks},
  lccn = {P98.5.S83 L487 2015}
}

@article{liaw_classification_2002,
  title = {Classification and Regression by {{randomForest}}},
  author = {Liaw, Andy and Wiener, Matthew},
  year = {2002},
  volume = {2},
  pages = {18--22},
  file = {C\:\\Users\\grafmilj\\Zotero\\storage\\9JEQXH6H\\Liaw_Wiener_2002-Classification_and_Regression_by_RandomForest.pdf},
  journal = {R news},
  keywords = {R (Computer program language),R-package,random forests},
  number = {3}
}

@article{loecher_unbiased_2020,
  title = {Unbiased Variable Importance for Random Forests},
  author = {Loecher, Markus},
  year = {2020},
  month = may,
  volume = {0},
  pages = {1--13},
  publisher = {{Taylor \& Francis}},
  issn = {0361-0926},
  doi = {10.1080/03610926.2020.1764042},
  abstract = {The default variable-importance measure in random forests, Gini importance, has been shown to suffer from the bias of the underlying Gini-gain splitting criterion. While the alternative permutation importance is generally accepted as a reliable measure of variable importance, it is also computationally demanding and suffers from other shortcomings. We propose a simple solution to the misleading/untrustworthy Gini importance which can be viewed as an over-fitting problem: we compute the loss reduction on the out-of-bag instead of the in-bag training samples.},
  annotation = {00000  \_eprint: https://doi.org/10.1080/03610926.2020.1764042},
  file = {C\:\\Users\\grafmilj\\Zotero\\storage\\KLICDT7C\\Loecher - 2020 - Unbiased variable importance for random forests.pdf;C\:\\Users\\grafmilj\\Zotero\\storage\\GEBESY7P\\03610926.2020.html},
  journal = {Communications in Statistics - Theory and Methods},
  keywords = {Gini impurity,random forests,trees,Variable importance},
  number = {0}
}

@book{mcelreath_statistical_2020,
  title = {Statistical Rethinking: A {{Bayesian}} Course with Examples in {{R}} and {{Stan}}},
  shorttitle = {Statistical Rethinking},
  author = {McElreath, Richard},
  year = {2020},
  edition = {Second},
  publisher = {{Taylor and Francis, CRC Press}},
  address = {{Boca Raton}},
  abstract = {"Statistical Rethinking: A Bayesian Course with Examples in R and Stan, Second Edition builds knowledge/confidence in statistical modeling. Pushes readers to perform step-by-step calculations (usually automated.) Unique, computational approach ensures readers understand details to make reasonable choices and interpretations in their modeling work"--},
  annotation = {00836},
  isbn = {978-0-367-13991-9},
  keywords = {Bayesian methods,Bayesian modeling,textbooks},
  series = {{{CRC}} Texts in Statistical Science}
}

@book{silge_text_2017a,
  title = {Text Mining with {{R}}: A Tidy Approach},
  shorttitle = {Text Mining with {{R}}},
  author = {Silge, Julia and Robinson, David},
  year = {2017},
  edition = {First edition},
  publisher = {{O'Reilly}},
  address = {{Beijing ; Boston}},
  annotation = {00351  OCLC: ocn993582128},
  isbn = {978-1-4919-8165-8},
  keywords = {Data mining,R (Computer program language),sentiment analysis,text mining,textbooks},
  lccn = {QA76.9.D343 S5935 2017}
}

@article{speiser_bimm_2018,
  title = {{{BiMM}} Tree: A Decision Tree Method for Modeling Clustered and Longitudinal Binary Outcomes},
  shorttitle = {{{BiMM}} Tree},
  author = {Speiser, Jaime Lynn and Wolf, Bethany J. and Chung, Dongjun and Karvellas, Constantine J. and Koch, David G. and Durkalski, Valerie L.},
  year = {2018},
  month = sep,
  volume = {0},
  pages = {1--20},
  issn = {0361-0918},
  doi = {10.1080/03610918.2018.1490429},
  abstract = {Clustered binary outcomes are frequently encountered in clinical research (e.g. longitudinal studies). Generalized linear mixed models (GLMMs) for clustered endpoints have challenges for some scenarios (e.g. data with multi-way interactions and nonlinear predictors unknown a priori). We develop an alternative, data-driven method called Binary Mixed Model (BiMM) tree, which combines decision tree and GLMM within a unified framework. Simulation studies show that BiMM tree achieves slightly higher or similar accuracy compared to standard methods. The method is applied to a real dataset from the Acute Liver Failure Study Group.},
  annotation = {00001},
  file = {C\:\\Users\\grafmilj\\Zotero\\storage\\LB37KZPA\\Speiser et al. - 2018 - BiMM tree a decision tree method for modeling clu.pdf;C\:\\Users\\grafmilj\\Zotero\\storage\\UEIFUHPS\\03610918.2018.html},
  journal = {Communications in Statistics - Simulation and Computation},
  keywords = {CARTs,Classification and regression tree,clustered data,decision tree,Decision trees,longitudinal data,Mixed-effects modeling,to read,Tree based methods},
  number = {0}
}

@article{strobl_bias_2007,
  title = {Bias in Random Forest Variable Importance Measures: {{Illustrations}}, Sources and a Solution},
  shorttitle = {Bias in {{Random Forest Variable Importance Measures}}},
  author = {Strobl, Carolin and Boulesteix, Anne-Laure and Zeileis, Achim and Hothorn, Torsten},
  year = {2007},
  month = jan,
  volume = {8},
  pages = {25},
  issn = {1471-2105},
  doi = {10.1186/1471-2105-8-25},
  abstract = {Variable importance measures for random forests have been receiving increased attention as a means of variable selection in many classification tasks in bioinformatics and related scientific fields, for instance to select a subset of genetic markers relevant for the prediction of a certain disease. We show that random forest variable importance measures are a sensible means for variable selection in many applications, but are not reliable in situations where potential predictor variables vary in their scale of measurement or their number of categories. This is particularly important in genomics and computational biology, where predictors often include variables of different types, for example when predictors include both sequence data and continuous variables such as folding energy, or when amino acid sequence data show different numbers of categories. PMID: 17254353},
  annotation = {01955},
  copyright = {2007 Strobl et al; licensee BioMed Central Ltd.},
  journal = {BMC Bioinformatics},
  keywords = {random forests},
  language = {en},
  number = {1},
  pmid = {17254353}
}

@article{strobl_conditional_2008,
  title = {Conditional Variable Importance for Random Forests},
  author = {Strobl, Carolin and Boulesteix, Anne-Laure and Kneib, Thomas and Augustin, Thomas and Zeileis, Achim},
  year = {2008},
  volume = {9},
  pages = {307},
  issn = {1471-2105},
  doi = {10.1186/1471-2105-9-307},
  annotation = {02010},
  journal = {BMC Bioinformatics},
  keywords = {R (Computer program language),R-package,random forests},
  language = {en},
  number = {1}
}

@article{strobl_introduction_2009,
  title = {An Introduction to Recursive Partitioning: {{Rationale}}, Application, and Characteristics of Classification and Regression Trees, Bagging, and Random Forests},
  author = {Strobl, Carolin and Malley, James and Tutz, Gerhard},
  year = {2009},
  volume = {14},
  pages = {323--348},
  doi = {10.1037/a0016973},
  file = {C\:\\Users\\grafmilj\\Zotero\\storage\\3TJS24V9\\Strobl_et_al_2009.pdf},
  journal = {Psychological Methods},
  keywords = {modeling,random forests,statistics},
  number = {4}
}

@article{szmrecsanyi_world_2016,
  title = {Around the World in Three Alternations: {{Modeling}} Syntactic Variation in Varieties of {{English}}},
  shorttitle = {Around the World in Three Alternations},
  author = {Szmrecsanyi, Benedikt and Grafmiller, Jason and Heller, Benedikt and R{\"o}thlisberger, Melanie},
  year = {2016},
  volume = {37},
  pages = {109--137},
  issn = {0172-8865, 1569-9730},
  doi = {10.1075/eww.37.2.01szm},
  file = {C\:\\Users\\grafmilj\\Zotero\\storage\\UDU3ZGCM\\Szmrecsanyi_et_al_2016-Around_the_World.pdf},
  journal = {English World-Wide},
  keywords = {Comparative sociolinguistics,datives,genitives,Particle-placement,sociolinguistics,syntactic variation,world-Englishes},
  language = {en},
  number = {2}
}

@article{tagliamonte_models_2012,
  title = {Models, Forests and Trees of {{York English}}: {\emph{Was/Were}} Variation as a Case Study for Statistical Practice},
  author = {Tagliamonte, Sali and Baayen, Harald},
  year = {2012},
  volume = {24},
  pages = {135--178},
  doi = {10.1017/S0954394512000129},
  file = {C\:\\Users\\grafmilj\\Zotero\\storage\\MRBJZVFM\\TagliamonteBaayen(2012).pdf},
  journal = {Language Variation and Change},
  keywords = {modeling,random forests,statistics},
  number = {2}
}

@article{tagliamonte_outliers_2016,
  title = {Outliers, Impact, and Rationalization in Linguistic Change},
  author = {Tagliamonte, Sali A. and D'Arcy, Alexandra and Louro, Celeste Rodr{\'i}guez},
  year = {2016},
  volume = {92},
  pages = {824--849},
  issn = {1535-0665},
  doi = {10/gdg6vt},
  annotation = {00008},
  file = {C\:\\Users\\grafmilj\\Zotero\\storage\\A9KPL53Q\\Tagliamonte et al. - 2016 - Outliers, impact, and rationalization in linguisti.pdf},
  journal = {Language},
  keywords = {CARTs,language change,quotative-verbs,sociolinguistics},
  language = {en},
  number = {4}
}

@article{tweedie_how_1998,
  title = {How Variable May a Constant Be? {{Measures}} of Lexical Richness in Perspective},
  author = {Tweedie, Fiona J. and Baayen, Harald},
  year = {1998},
  volume = {32},
  pages = {323--352},
  annotation = {00507},
  journal = {Computers and the Humanities},
  keywords = {lexical richness,statistics},
  number = {5}
}

@article{wei_variable_2015,
  title = {Variable Importance Analysis: {{A}} Comprehensive Review},
  shorttitle = {Variable Importance Analysis},
  author = {Wei, Pengfei and Lu, Zhenzhou and Song, Jingwen},
  year = {2015},
  month = oct,
  volume = {142},
  pages = {399--432},
  issn = {0951-8320},
  doi = {10.1016/j.ress.2015.05.018},
  abstract = {Measuring variable importance for computational models or measured data is an important task in many applications. It has drawn our attention that the variable importance analysis (VIA) techniques were developed independently in many disciplines. We are strongly aware of the necessity to aggregate all the good practices in each discipline, and compare the relative merits of each method, so as to instruct the practitioners to choose the optimal methods to meet different analysis purposes, and to guide current research on VIA. To this end, all the good practices, including seven groups of methods, i.e., the difference-based variable importance measures (VIMs), parametric regression and related VIMs, nonparametric regression techniques, hypothesis test techniques, variance-based VIMs, moment-independent VIMs and graphic VIMs, are reviewed and compared with a numerical test example set in two situations (independent and dependent cases). For ease of use, the recommendations are provided for different types of applications, and packages as well as software for implementing these VIA techniques are collected. Prospects for future study of VIA techniques are also proposed.},
  annotation = {00061},
  file = {C\:\\Users\\grafmilj\\Zotero\\storage\\V3PYS2EZ\\Wei et al. - 2015 - Variable importance analysis A comprehensive revi.pdf;C\:\\Users\\grafmilj\\Zotero\\storage\\SQSW8PG9\\S0951832015001672.html},
  journal = {Reliability Engineering \& System Safety},
  keywords = {CARTs,Difference-based,Graphic variable importance measures,Moment-independent,random forests,Regression technique,Variable importance analysis,Variance-based}
}

@book{wickham_advanced_2019,
  title = {Advanced {{R}}},
  author = {Wickham, Hadley},
  year = {2019},
  edition = {Second edition},
  publisher = {{CRC Press/Taylor and Francis Group}},
  address = {{Boca Raton}},
  annotation = {00300},
  isbn = {978-1-351-20129-2},
  keywords = {R (Computer program language),textbooks},
  lccn = {QA276.45.R3}
}

@book{wickham_data_2016,
  title = {R for Data Science: Import, Tidy, Transform, Visualize, and Model Data},
  shorttitle = {R for Data Science},
  author = {Wickham, Hadley and Grolemund, Garrett},
  year = {2016},
  edition = {First edition},
  publisher = {{O'Reilly}},
  address = {{Sebastopol, CA}},
  abstract = {"This book introduces you to R, RStudio, and the tidyverse, a collection of R packages designed to work together to make data science fast, fluent, and fun. Suitable for readers with no previous programming experience"--},
  annotation = {00445  OCLC: ocn968213225},
  isbn = {978-1-4919-1039-9 978-1-4919-1036-8},
  keywords = {Big data,Computer programs,Data mining,Databases,Electronic data processing,Information visualization,R (Computer program language),Statistics,textbooks},
  lccn = {QA276.45.R3 W53 2016}
}

@book{winter_statistics_2019,
  title = {Statistics for Linguists: An Introduction Using {{R}}},
  shorttitle = {Statistics for Linguists},
  author = {Winter, Bodo},
  year = {2019},
  publisher = {{Routledge}},
  address = {{New York, NY}},
  abstract = {"Statistics for Linguists: An introduction using R is the first statistics textbook on linear models for linguistics. The book covers simple uses of linear models through generalized models to more advanced approaches, maintaining its focus on conceptual issues and avoiding excessive mathematical details. It contains many applied examples using the R statistical programming environment. Written in an accessible tone and style, this text is the ideal main resource for graduate and advanced undergraduate students of Linguistics statistics courses as well as those in other fields including Psychology, Cognitive Science, and Data Science"--},
  annotation = {00005},
  isbn = {978-1-315-16554-7},
  keywords = {Linguistics,Mathematical linguistics,R (Computer program language),regression,Statistical methods,textbooks},
  lccn = {P138.5}
}

@article{wright_little_2016,
  title = {Do Little Interactions Get Lost in Dark Random Forests?},
  author = {Wright, Marvin N. and Ziegler, Andreas and K{\"o}nig, Inke R.},
  year = {2016},
  month = dec,
  volume = {17},
  issn = {1471-2105},
  doi = {10/b5t7},
  annotation = {00000},
  file = {C\:\\Users\\grafmilj\\Zotero\\storage\\UVP4Z6QM\\Wright et al. - 2016 - Do little interactions get lost in dark random for.pdf},
  journal = {BMC Bioinformatics},
  keywords = {random forests},
  language = {en},
  number = {1}
}

@article{wright_ranger_2017,
  title = {{\textbf{Ranger}} : {{A Fast Implementation}} of {{Random Forests}} for {{High Dimensional Data}} in {{{\emph{C}}}}{\emph{++}} and {{{\emph{R}}}}},
  shorttitle = {{\textbf{Ranger}}},
  author = {Wright, Marvin N. and Ziegler, Andreas},
  year = {2017},
  volume = {77},
  issn = {1548-7660},
  doi = {10.18637/jss.v077.i01},
  annotation = {00000},
  file = {C\:\\Users\\grafmilj\\Zotero\\storage\\LZ7ISVA5\\Wright and Ziegler - 2017 - brangerb  A Fast Implementation of Random Fo.pdf},
  journal = {Journal of Statistical Software},
  keywords = {R-package,random forests},
  language = {en},
  number = {1}
}

@article{zhu_reinforcement_2015,
  title = {Reinforcement {{Learning Trees}}},
  author = {Zhu, Ruoqing and Zeng, Donglin and Kosorok, Michael R.},
  year = {2015},
  month = oct,
  volume = {110},
  pages = {1770--1784},
  issn = {0162-1459},
  doi = {10/gfz9vk},
  abstract = {In this article, we introduce a new type of tree-based method, reinforcement learning trees (RLT), which exhibits significantly improved performance over traditional methods such as random forests (Breiman 2001) under high-dimensional settings. The innovations are three-fold. First, the new method implements reinforcement learning at each selection of a splitting variable during the tree construction processes. By splitting on the variable that brings the greatest future improvement in later splits, rather than choosing the one with largest marginal effect from the immediate split, the constructed tree uses the available samples in a more efficient way. Moreover, such an approach enables linear combination cuts at little extra computational cost. Second, we propose a variable muting procedure that progressively eliminates noise variables during the construction of each individual tree. The muting procedure also takes advantage of reinforcement learning and prevents noise variables from being considered in the search for splitting rules, so that toward terminal nodes, where the sample size is small, the splitting rules are still constructed from only strong variables. Last, we investigate asymptotic properties of the proposed method under basic assumptions and discuss rationale in general settings. Supplementary materials for this article are available online.},
  annotation = {00055},
  file = {C\:\\Users\\grafmilj\\Zotero\\storage\\7W8MGI8A\\01621459.2015.html},
  journal = {Journal of the American Statistical Association},
  keywords = {Consistency,Error bound,random forests,Reinforcement learning,to read,Tree based methods,Trees},
  number = {512},
  pmid = {26903687}
}


